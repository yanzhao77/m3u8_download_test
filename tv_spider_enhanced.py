import os
import json
import subprocess
import time
import random
import requests
from lxml import html
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Pool, cpu_count
from playwright.sync_api import sync_playwright

# ============================================
# 1. å¢å¼ºç‰ˆå¤šæºå…è´¹ä»£ç†æ± ï¼ˆæ”¯æŒ HTTPSï¼‰
# ============================================

def test_proxy(proxy_url, timeout=3):
    try:
        resp = requests.get(
            "https://xiaoxintv.cc/",
            proxies={"http": proxy_url, "https": proxy_url},
            timeout=timeout,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        return resp.status_code == 200
    except:
        return False

def scrape_free_proxy_list():
    proxies = []
    try:
        resp = requests.get("https://free-proxy-list.net/", timeout=10)
        tree = html.fromstring(resp.content)
        rows = tree.xpath('//table[contains(@class,"table")]//tbody/tr')
        for row in rows:
            ip = ''.join(row.xpath('./td[1]/text()')).strip()
            port = ''.join(row.xpath('./td[2]/text()')).strip()
            https = ''.join(row.xpath('./td[7]/text()')).strip()
            if ip and port and https == 'yes':
                proxies.append(f"http://{ip}:{port}")
    except Exception as e:
        print(f"[free-proxy-list.net] æŠ“å–å¤±è´¥: {e}")
    return proxies

def scrape_geonode():
    proxies = []
    try:
        resp = requests.get(
            "https://proxylist.geonode.com/api/proxy-list?limit=200&sort_by=lastChecked&sort_type=desc",
            timeout=10
        )
        data = resp.json()
        for item in data.get('data', []):
            if item.get('protocols') and 'https' in item['protocols']:
                ip = item['ip']
                port = item['port']
                proxies.append(f"http://{ip}:{port}")
    except Exception as e:
        print(f"[geonode.com] æŠ“å–å¤±è´¥: {e}")
    return proxies

def scrape_spys_one():
    proxies = []
    try:
        resp = requests.get("http://spys.me/proxy.txt", timeout=10)
        for line in resp.text.splitlines():
            if ":" in line and ("H" in line or "S" in line):
                parts = line.split()
                if len(parts) >= 2:
                    ip_port = parts[0]
                    if ip_port.replace('.', '').replace(':', '').isdigit():
                        proxies.append(f"http://{ip_port}")
    except Exception as e:
        print(f"[spys.me] æŠ“å–å¤±è´¥: {e}")
    return proxies

def scrape_proxy_scrape():
    proxies = []
    try:
        resp = requests.get(
            "https://api.proxyscrape.com/v3/free-proxy-list/get?"
            "request=displayproxies&protocol=http&timeout=5000&country=CN,US,SG,HK&anonymity=elite&ssl=yes&format=text",
            timeout=10
        )
        for line in resp.text.strip().splitlines():
            if ':' in line:
                proxies.append(f"http://{line.strip()}")
    except Exception as e:
        print(f"[proxyscrape.com] æŠ“å–å¤±è´¥: {e}")
    return proxies

def scrape_89ip():
    proxies = []
    try:
        resp = requests.get("http://www.89ip.cn/tqdl.html?num=30&address=&kill_address=&port=&kill_port=&isp=")
        for line in resp.text.split('<br>'):
            line = line.strip()
            if ':' in line and line.replace('.', '').replace(':', '').replace('\n', '').isdigit():
                proxies.append(f"http://{line}")
    except Exception as e:
        print(f"[89ip.cn] æŠ“å–å¤±è´¥: {e}")
    return proxies

def fetch_enhanced_proxies(max_proxies=12, test_timeout=3):
    all_candidates = set()
    scrapers = [scrape_free_proxy_list, scrape_geonode, scrape_spys_one, scrape_proxy_scrape, scrape_89ip]

    print("ğŸŒ æ­£åœ¨ä»å¤šä¸ªå…è´¹æºå¹¶è¡ŒæŠ“å–ä»£ç†...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(scraper): scraper.__name__ for scraper in scrapers}
        for future in as_completed(futures):
            try:
                proxies = future.result()
                all_candidates.update(proxies)
            except:
                pass

    print(f"ğŸ“¥ å…±æ”¶é›†åˆ° {len(all_candidates)} ä¸ªå€™é€‰ä»£ç†ï¼Œæ­£åœ¨æµ‹è¯•å¯ç”¨æ€§...")
    valid_proxies = []
    with ThreadPoolExecutor(max_workers=min(20, len(all_candidates))) as executor:
        future_to_proxy = {
            executor.submit(test_proxy, proxy, test_timeout): proxy
            for proxy in all_candidates
        }
        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                if future.result():
                    valid_proxies.append(proxy)
                    if len(valid_proxies) >= max_proxies:
                        pass
            except:
                continue

    valid_proxies = list(set(valid_proxies))
    random.shuffle(valid_proxies)
    result = valid_proxies[:max_proxies]
    print(f"âœ… æœ€ç»ˆè·å¾— {len(result)} ä¸ªå¯ç”¨ HTTPS ä»£ç†")
    return result

def get_m3u8_with_retry(play_url, proxy_pool, max_retries=3):
    """
    å°è¯•ä½¿ç”¨ä¸åŒä»£ç†è·å– m3u8ï¼Œç›´åˆ°æˆåŠŸæˆ–é‡è¯•è€—å°½
    """
    for attempt in range(max_retries + 1):
        if attempt == 0:
            # ç¬¬ä¸€æ¬¡ï¼šä¼˜å…ˆç”¨ä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
            proxy = random.choice(proxy_pool) if proxy_pool else None
        else:
            # é‡è¯•ï¼šå¦‚æœè¿˜æœ‰å…¶ä»–ä»£ç†ï¼Œæ¢ä¸€ä¸ªï¼›å¦åˆ™ç”¨æœ¬æœº IP
            if proxy_pool and len(proxy_pool) > 1:
                # æ’é™¤ä¸Šæ¬¡ç”¨çš„ï¼ˆç®€å•åšæ³•ï¼šé‡æ–°éšæœºé€‰ï¼‰
                proxy = random.choice(proxy_pool)
            else:
                proxy = None  # æ”¹ç”¨æœ¬æœº IP

        print(f"ğŸ” å°è¯•ç¬¬ {attempt + 1}/{max_retries + 1} æ¬¡ï¼ˆä»£ç†: {proxy}ï¼‰")

        try:
            m3u8_list = get_m3u8(play_url, proxy=proxy)
            if m3u8_list:
                return m3u8_list
        except Exception as e:
            print(f"âš ï¸ ä»£ç† {proxy} å¤±è´¥: {e}")

        # é‡è¯•å‰ç­‰å¾…ï¼ˆé¿å…å¤ªå¿«ï¼‰
        if attempt < max_retries:
            wait = random.uniform(2, 5)
            print(f"â³ ç­‰å¾… {wait:.1f} ç§’åé‡è¯•...")
            time.sleep(wait)

    print("ğŸ’€ æ‰€æœ‰ä»£ç†å°è¯•å¤±è´¥ï¼Œæ”¾å¼ƒè·å– m3u8")
    return []

def find_page_with_retry(root_page, proxy_pool, max_retries=2):
    for attempt in range(max_retries + 1):
        proxy = random.choice(proxy_pool) if proxy_pool and attempt == 0 else None
        if attempt > 0:
            proxy = None  # é‡è¯•æ—¶ç”¨æœ¬æœº
        try:
            print(f"ğŸ“š è·å–å‰§é›†åˆ—è¡¨ - å°è¯• {attempt + 1}ï¼ˆä»£ç†: {proxy}ï¼‰")
            return find_page(root_page, proxy=proxy)
        except Exception as e:
            print(f"âš ï¸ å‰§é›†åˆ—è¡¨é¡µå¤±è´¥ (ä»£ç† {proxy}): {e}")
            if attempt < max_retries:
                time.sleep(random.uniform(3, 6))
    raise Exception("å‰§é›†åˆ—è¡¨é¡µæ‰€æœ‰å°è¯•å‡å¤±è´¥")

# ============================================
# 2. è·å–å‰§é›†æ’­æ”¾é¡µï¼ˆå¸¦ä»£ç†ï¼‰
# ============================================
def find_page(url, proxy=None):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, slow_mo=100)
        context_kwargs = {
            "viewport": {"width": 1920, "height": 1080},
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        }
        if proxy:
            context_kwargs["proxy"] = {"server": proxy}
        context = browser.new_context(**context_kwargs)
        page = context.new_page()

        print(f"â³ æ‰“å¼€å‰§é›†é¡µï¼ˆä»£ç†: {proxy}ï¼‰ï¼š{url}")
        page.goto(url)
        page.wait_for_selector("#playlist", timeout=30000)

        li_list = page.query_selector_all('//*[@id="playlist"]/li')
        href_map = {}
        for li in li_list:
            a = li.query_selector("a")
            if a:
                name = " ".join(a.inner_text().strip().split())  # åˆå¹¶å¤šä½™ç©ºç™½
                href = a.get_attribute("href")
                href_map[name] = href

        browser.close()
        return href_map

# ============================================
# 3. è·å– m3u8ï¼ˆå¸¦ä»£ç† + åæ£€æµ‹ï¼‰
# ============================================
def get_m3u8(url, proxy=None):
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-web-security",
            ]
        )

        context_kwargs = {
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "viewport": {"width": 1920, "height": 1080},
            "locale": "zh-CN",
            "timezone_id": "Asia/Shanghai",
            "permissions": ["notifications"],
            "bypass_csp": True,
        }
        if proxy:
            context_kwargs["proxy"] = {"server": proxy}

        context = browser.new_context(**context_kwargs)
        page = context.new_page()

        page.add_init_script("""
            delete navigator.__proto__.webdriver;
            window.chrome = { runtime: {} };
            Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh'] });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
        """)

        m3u8_list = []

        def on_request(request):
            if ".m3u8" in request.url and request.url not in m3u8_list:
                print("ğŸ¯ æ•è·åˆ° M3U8:", request.url)
                m3u8_list.append(request.url)

        page.on("request", on_request)

        print(f"â³ åŠ è½½æ’­æ”¾é¡µï¼ˆä»£ç†: {proxy}ï¼‰ï¼š{url}")
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
        except Exception as e:
            print("âŒ å¯¼èˆªå¤±è´¥:", e)
            browser.close()
            return []

        print("ğŸ•’ ç­‰å¾… Cloudflare éªŒè¯å’Œè§†é¢‘åŠ è½½...")
        start = time.time()
        while time.time() - start < 30:
            if m3u8_list:
                break
            try:
                content = page.content()
                if "cf-challenge" not in content and "Checking if" not in content:
                    page.wait_for_timeout(3000)
                    break
            except:
                pass
            page.wait_for_timeout(1000)

        browser.close()
        return m3u8_list

# ============================================
# 4. å¤šè¿›ç¨‹ä¸‹è½½ï¼ˆyt-dlpï¼‰
# ============================================
def download_one(args):
    ep_name, m3u8_url, save_dir = args
    output_path = os.path.join(save_dir, ep_name + ".mp4")

    cmd = [
        "yt-dlp",
        "-N", "16",
        "-f", "bestvideo+bestaudio/best",
        "--no-check-certificate",
        "--retries", "3",
        "--user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "-o", output_path,
        m3u8_url
    ]

    print(f"\nâ¬‡ï¸ å¼€å§‹ä¸‹è½½ï¼š{ep_name} â†’ {output_path}")

    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šä¸å† capture_outputï¼Œè€Œæ˜¯è®©è¾“å‡ºç›´æ¥æ˜¾ç¤º
    result = subprocess.run(cmd, stdout=None, stderr=None)  # ç»§æ‰¿çˆ¶è¿›ç¨‹çš„ stdout/stderr

    if result.returncode == 0:
        print(f"\nâœ… ä¸‹è½½æˆåŠŸï¼š{ep_name}")
    else:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥ï¼š{ep_name}ï¼ˆé€€å‡ºç : {result.returncode}ï¼‰")
    return result.returncode == 0

def parallel_download(m3u8_map, save_dir):
    tasks = [(ep, url, save_dir) for ep, url in m3u8_map.items() if url]
    if not tasks:
        print("âš ï¸ æ— æœ‰æ•ˆ m3u8ï¼Œè·³è¿‡ä¸‹è½½")
        return
    workers = min(len(tasks), cpu_count(), 4)  # é™åˆ¶æœ€å¤š 4 è¿›ç¨‹é˜²å¡æ­»

    print(f"\nğŸ”¥ å¤šè¿›ç¨‹ä¸‹è½½ï¼š{workers} workers\n")
    with Pool(workers) as pool:
        pool.map(download_one, tasks)

# ============================================
# 5. ä¸‹è½½ä¸€æ•´å­£ï¼ˆä¸»é€»è¾‘ï¼‰
# ============================================
def download_main(save_dir, base_url, root_page, base_name, proxy_pool):
    save_season = os.path.join(save_dir, base_name)
    os.makedirs(save_season, exist_ok=True)

    print(f"\n================ {base_name} =================")

    # ğŸ” æ­¥éª¤1: æ£€æŸ¥å·²ä¸‹è½½çš„é›†ï¼ˆé€šè¿‡ .mp4 æ–‡ä»¶åï¼‰
    existing_episodes = set()
    for file in os.listdir(save_season):
        if file.endswith(".mp4"):
            # ç§»é™¤ .mp4 åç¼€ï¼Œå¾—åˆ°é›†åï¼ˆå¦‚ "ç¬¬1é›†"ï¼‰
            ep_name = os.path.splitext(file)[0]
            existing_episodes.add(ep_name)
    print(f"ğŸ“ å·²å­˜åœ¨ {len(existing_episodes)} é›†ï¼Œå°†è·³è¿‡")

    # ğŸ” æ­¥éª¤2: è·å–å‰§é›†åˆ—è¡¨ï¼ˆå¸¦ä»£ç†ï¼‰
    # è·å–å‰§é›†åˆ—è¡¨å‰ä¹ŸåŠ å»¶è¿Ÿï¼ˆå°¤å…¶æ— ä»£ç†æ—¶ï¼‰
    list_delay = random.uniform(5, 10) if proxy_pool else random.uniform(10, 20)
    print(f"â³ ç­‰å¾… {list_delay:.1f} ç§’åè·å–å‰§é›†åˆ—è¡¨ï¼ˆ{'æœ‰ä»£ç†' if proxy_pool else 'æ— ä»£ç†æ¨¡å¼'}ï¼‰...")
    time.sleep(list_delay)

    list_proxy = random.choice(proxy_pool) if proxy_pool else None
    try:
        ep_map = find_page_with_retry(root_page, proxy_pool)
    except Exception as e:
        print(f"âŒ è·å–å‰§é›†åˆ—è¡¨å¤±è´¥: {e}")
        return

    # ğŸ—‚ï¸ æ­¥éª¤3: è¿‡æ»¤æœªä¸‹è½½çš„é›†
    missing_episodes = {
        ep_name: ep_path
        for ep_name, ep_path in ep_map.items()
        if ep_name not in existing_episodes
    }

    if not missing_episodes:
        print("ğŸ‰ æœ¬å­£æ‰€æœ‰å‰§é›†å·²ä¸‹è½½ï¼Œè·³è¿‡ï¼")
        return

    print(f"ğŸ“¥ æœ¬å­£éœ€ä¸‹è½½ {len(missing_episodes)} é›†ï¼š{list(missing_episodes.keys())}")

    all_m3u8 = {}

    # ğŸ”„ åªå¤„ç†ç¼ºå¤±çš„é›†
    for ep_name, ep_path in missing_episodes.items():
        # æ ¹æ®æ˜¯å¦æœ‰ä»£ç†åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
        if proxy_pool:
            delay = random.uniform(3, 8)  # æœ‰ä»£ç†ï¼šæ­£å¸¸å»¶è¿Ÿ
        else:
            delay = random.uniform(8, 15)  # æ— ä»£ç†ï¼šæ›´é•¿å»¶è¿Ÿï¼Œé™ä½é£æ§
        print(f"\nâ¸ï¸ äººå·¥ç­‰å¾… {delay:.1f} ç§’ï¼ˆ{'æœ‰ä»£ç†' if proxy_pool else 'æ— ä»£ç†æ¨¡å¼'}ï¼‰...")
        time.sleep(delay)

        play_url = base_url + ep_path
        print(f"\n====== å¤„ç† {ep_name} ======")
        print(f"æ’­æ”¾é¡µï¼š{play_url}")

        m3u8 = ""
        try:
            m3u8_list = get_m3u8_with_retry(play_url, proxy_pool, max_retries=2)
            if not m3u8_list:
                print("âŒ æ²¡æœ‰æ‰¾åˆ° m3u8")
                continue
            for url in m3u8_list:
                if url.endswith(".m3u8"):
                    m3u8 = url
                    break
            if not m3u8:
                m3u8 = m3u8_list[0]
        except Exception as e:
            print(f"[ERROR] æŠ“å–å¤±è´¥: {ep_name}, {e}")
            continue

        all_m3u8[ep_name] = m3u8
        print(f"ğŸ¯ {ep_name} æœ€ç»ˆ m3u8ï¼š{m3u8}")

    if not all_m3u8:
        print("âš ï¸ æ— æ–° m3u8 å¯ä¸‹è½½")
        return

    # ğŸ’¾ ä¿å­˜ m3u8 åˆ—è¡¨ï¼ˆå¯é€‰ï¼šè¿½åŠ æˆ–è¦†ç›–ï¼‰
    json_path = os.path.join(save_season, "m3u8_list.json")
    combined = {}
    if os.path.exists(json_path):
        with open(json_path, "r", encoding="utf-8") as f:
            combined = json.load(f)
    combined.update(all_m3u8)
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(combined, f, ensure_ascii=False, indent=4)
    print("\nğŸ“„ m3u8 åˆ—è¡¨å·²æ›´æ–°ï¼š", json_path)

    # â–¶ï¸ ä¸‹è½½ç¼ºå¤±é›†
    parallel_download(all_m3u8, save_season)

# ============================================
# 6. ä¸»ç¨‹åºå…¥å£
# ============================================
if __name__ == "__main__":
    # ğŸ”§ é…ç½®åŒº
    base_url = "https://xiaoxintv.cc/"  # âœ… å·²ä¿®æ­£
    base_name = "ç”Ÿæ´»å¤§çˆ†ç‚¸"
    save_dir = os.path.join("E:\\video", base_name)
    os.makedirs(save_dir, exist_ok=True)

    total_season = 12
    num = 205590
    root_tpl = "https://xiaoxintv.cc/index.php/vod/play/id/{num}/sid/1/nid/1.html"

    # ğŸŒ è·å–å¢å¼ºä»£ç†æ± 
    proxy_pool = fetch_enhanced_proxies(max_proxies=12)
    if not proxy_pool:
        print("âš ï¸ è­¦å‘Šï¼šæœªè·å–åˆ°å¯ç”¨ä»£ç†ï¼å°†ä½¿ç”¨æœ¬åœ° IPï¼ˆé«˜é£é™©ï¼‰")
        proxy_pool = []

    # ğŸ“º æ„é€ æ‰€æœ‰å­£
    all_pages = {
        f"{base_name} ç¬¬{idx}å­£": root_tpl.format(num=num - (idx - 1))
        for idx in range(1, total_season + 1)
    }

    # â–¶ï¸ å¼€å§‹ä¸‹è½½
    for season_name, url in all_pages.items():
        print("\n" + "="*60)
        print(f"ğŸ¬ å¼€å§‹ä¸‹è½½ï¼š{season_name}")
        print(f"ğŸ”— URL: {url}")
        try:
            download_main(save_dir, base_url, url, season_name, proxy_pool)
        except Exception as e:
            print(f"ğŸ’¥ å­£ {season_name} å®Œå…¨å¤±è´¥: {e}")
            continue

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")